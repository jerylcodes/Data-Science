{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: My Grocery Finder  (myGF)\n",
    "\n",
    "cap1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "Do you ever remember times where you have spent a large amount of time finding a grocery that you need at a particularly large supermarket? Groceries in a supermarket are organised into rows and categories. It is often difficult to pin point the exact location of the particular grocery that you need.  \n",
    "\n",
    "In today's technologial landscape, we are able to leverage techniques such as **deep learning** to easily classify images or even speech. For this use case, we will make use of **Convolutional Neural Networks** (CNN) and transfer learning to help with the classification of groceries. This can then add value to supermarkets in the form of helping grocers find their groceries in a shorter time. A proposal might be to add kiosks around the supermarket with the image classifer. Grocers will only need to scan a photo or image of their grocery and the kiosk will return the row and column locations of the particular grocery.\n",
    "\n",
    "cap2\n",
    "Yay, candy can now be found easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Deep learning\n",
    "cap3  \n",
    "\n",
    "\n",
    "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. For humans, images are first processed by our eyes and the signal gets passed through a series of neurons before reaching our brain. Our brain then consolidates the image information and presents it in an understandble format for us.\n",
    "\n",
    "Similarly for deep learning in data science, data such as images are passed through layers that recognises salient features of that particular image. These features are then condensed and used to classify an image in our case. Specifically for this project, we will use a branch of deep learning - convolutional neural network to classify our groceries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convolutional Neural Networks\n",
    "\n",
    "In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks where images are broken down into number matrices and fed through layers for analysis in a forward direction.  \n",
    "\n",
    "CNN, in a nutshell, is made up of 4 main operations:\n",
    "1. Convolution\n",
    "2. Non Linearity (ReLU)\n",
    "3. Pooling or Sub Sampling\n",
    "4. Classification (Fully Connected Layer)\n",
    "\n",
    "cap4\n",
    "\n",
    "#### 1.2.1 Convolution\n",
    "The reason why CNN are called as such is because of the Convolution function. Images are first processed into pixel values (red, green and blue colour in the range of 0 to 255 for each colour channel). Each image is simply converted into a matrix of numbers. \n",
    "\n",
    "We will next define a matrix of 3x3 size with arbitrarily assigned weights to be used to slide across our image matrix. Dot products of the matrices will then be computed and this effectively reduces our original matrix into a feature map. The animation below explains how the convolution is performed and how the output is computed:\n",
    "\n",
    "cap5  \n",
    "\n",
    "Depending on the weights assigned, different feature maps will be obtained. These feature maps will then be used to classify our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Recitfied Linear Unit (ReLU) Activation\n",
    "\n",
    "An additional operation called ReLU has been used after every Convolution operation. ReLU stands for Rectified Linear Unit and is a non-linear operation. It simply replaces negative values with 0.\n",
    "\n",
    "ReLU is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero. The purpose of ReLU is to introduce non-linearity in our ConvNet. We would use ReLU for images as real world images consist of mostly non-linear features. The figure below illustrates the effect of ReLU activation on a processed image.\n",
    "\n",
    "cap 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Pooling\n",
    "\n",
    "Image matrices tend to be large. We would often apply pooling techniques to reduce the dimension of our feature maps. Max pooling is the typical pooling method we would use. In max pooling, we will first set a window (2x2 in this case) to slide through our feature map. Max pooling will take only the largest element for every step. Visually, the figure below shows how max pooling is performed:\n",
    "\n",
    "cap7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Fully Connected Layer\n",
    "The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer. The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer. The softmax activation function helps to reassign the computed values such that it adds up to 1, while highlighting the max value.  \n",
    "\n",
    "Fully connected layers often occur toward the end of a ConvNet. This is due to the fact that the output layer (final classification step) must be connected to all neurons of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training the ConvNet\n",
    "The ConvNet model is trained according to the following schema:\n",
    "\n",
    "- Step1: We initialize all filters and parameters / weights with random values\n",
    "\n",
    "- Step2: The network takes a training image as input, goes through the forward propagation step (convolution, ReLU and pooling operations along with forward propagation in the Fully Connected layer) and finds the output probabilities for each class.\n",
    "    - Lets say the output probabilities for the boat image above are [0.2, 0.4, 0.1, 0.3]\n",
    "    - Since weights are randomly assigned for the first training example, output probabilities are also random.  \n",
    "      \n",
    "      \n",
    "- Step3: Calculate the total error at the output layer (summation over all 4 classes)\n",
    "    - Total Error = ∑  ½ (target probability – output probability) ²  \n",
    "      \n",
    "      \n",
    "- Step4: Use Backpropagation to calculate the gradients of the error with respect to all weights in the network and use gradient descent to update all filter values / weights and parameter values to minimize the output error.\n",
    "    - The weights are adjusted in proportion to their contribution to the total error.\n",
    "    - When the same image is input again, output probabilities might now be [0.1, 0.1, 0.7, 0.1], which is closer to the target vector [0, 0, 1, 0].\n",
    "    - This means that the network has learnt to classify this particular image correctly by adjusting its weights / filters such that the output error is reduced.\n",
    "    - Parameters like number of filters, filter sizes, architecture of the network etc. have all been fixed before Step 1 and do not change during training process – only the values of the filter matrix and connection weights get updated.  \n",
    "      \n",
    "      \n",
    "- Step5: Repeat steps 2-4 with all images in the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps train the ConvNet – this essentially means that all the weights and parameters of the ConvNet have now been optimized to correctly classify images from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the training and the architecture of the ConvNet are as such:\n",
    "\n",
    "cap8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "The dataset used for this project is the GroZi-120 database from the University of California, San Diego. [Link to dataset](http://grozi.calit2.net/grozi.html). The database consists of photographs (training set) / google images of groceries. We will be building a ConvNet to classify 17 grocery items:\n",
    "1. Wiggly double mint\n",
    "2. Big red - gum\n",
    "3. Lays classic chips\n",
    "4. Pepperidge farm milano cookies\n",
    "5. Starbucks frappucino\n",
    "6. Oberto beef jerky\n",
    "7. Honey nut cheerios\n",
    "8. Tostitos scoops\n",
    "9. Ruffles sour cream chips\n",
    "10. Hersheys milk chocolate\n",
    "11. Twix cookie bar\n",
    "12. Snickers bar\n",
    "13. Tic tac winter\n",
    "14. Dentyne ice sweets\n",
    "15. Starburst\n",
    "16. Rockstar beer\n",
    "17. Nestle crunch\n",
    "\n",
    "cap 9\n",
    "\n",
    "### Packages used\n",
    "1. pandas\n",
    "2. numpy\n",
    "3. os\n",
    "4. cv2\n",
    "5. matplotlib/seaborn\n",
    "6. sci-kit learn\n",
    "7. keras (tensorflow-gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing images\n",
    "Images will have to be read in as a matrix. In order to increase computational efficiency and speed, images and downscaled to 128 x 128 pixels. 3 colour channels (red, green, blue) will be read into the matrix. Each image processed will result in a 128 x 128 x 3 matrix.  \n",
    "\n",
    "Since each grocery has only 100+ images for training, which is insufficient for building a ConvNet, we will make use of keras' data generator function, to augment the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "class_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509 509\n",
      "495 990\n",
      "465 930\n",
      "314 942\n",
      "273 819\n",
      "242 968\n",
      "215 860\n",
      "209 836\n",
      "203 812\n",
      "184 920\n",
      "174 870\n",
      "171 855\n",
      "148 888\n",
      "145 870\n",
      "120 960\n",
      "106 954\n",
      "104 936\n"
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "# Use counts for individual values, generate enough images till it hits roughly 1000\n",
    "for count in label_counts.values:\n",
    "    #nb of generations per image for this class label in order to make it size ~= class_size\n",
    "    ratio=math.floor(class_size/count)-1\n",
    "    print(count,count*(ratio+1))\n",
    "    dest_lab_dir=os.path.join(dest_train_dir,str(label_counts.index[it]))\n",
    "    src_lab_dir=os.path.join(src_train_dir,str(label_counts.index[it]))\n",
    "    if not os.path.exists(dest_lab_dir):\n",
    "        os.makedirs(dest_lab_dir)\n",
    "    for file in os.listdir(src_lab_dir):\n",
    "        img=load_img(os.path.join(src_lab_dir,file))\n",
    "        #img.save(os.path.join(dest_lab_dir,file))\n",
    "        x=img_to_array(img) \n",
    "        x=x.reshape((1,) + x.shape)\n",
    "        i=0\n",
    "        for batch in datagen.flow(x, batch_size=1,save_to_dir=dest_lab_dir, save_format='jpg'):\n",
    "            i+=1\n",
    "            if i > ratio:\n",
    "                break \n",
    "    it=it+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Number of images per grocery after augmentation and generation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3ff8e6cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD/CAYAAAD4xAEfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFbxJREFUeJzt3XuQnXV9x/H316RQ8UICWS7mwqJG\nLVapdCfQ2rG0KAZRQ1uYgh1JKW2mFS/VOhJrZ9LRaqO90NpW2tREoUNBpFrSlgqRS51eoITIJRg0\n20CTNQhrQdpKvQS//eP5ZTwsJ7t7Ltk9y+/9mjmzz/N7fud7fuds9nye3/M85yQyE0lSfZ4x2wOQ\nJM0OA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkio1f7YHMJlFixbl8PDw\nbA9DkuaUO+644+uZOTRVv4EOgOHhYbZu3Trbw5CkOSUi/nM6/TwEJEmVMgAkqVIGgCRVygCQpEoZ\nAJJUKQNAkiplAEhSpQwASarUQH8QrJ3htf8wrX4PrD/zII9EkuY2ZwCSVKkpAyAiNkXEwxGxvc22\nd0dERsSish4R8dGIGI2IuyPipJa+qyNiZ7mt7u/TkCR1ajozgE8CKyc2RsRS4DXA7pbmM4Dl5bYG\nuLT0PQJYB5wMrADWRcTCXgYuSerNlAGQmV8AHmmz6RLgPUC2tK0CLs/GrcCCiDgWeC2wJTMfycxH\ngS20CRVJ0szp6hxARLwR+Gpm3jVh02JgT8v6WGk7ULskaZZ0fBVQRBwGvA84vd3mNm05SXu7+mto\nDh+xbNmyTocnSZqmbmYALwCOB+6KiAeAJcC2iDiGZs9+aUvfJcDeSdqfIjM3ZOZIZo4MDU35/xlI\nkrrUcQBk5j2ZeVRmDmfmMM2b+0mZ+TVgM3B+uRroFOCxzHwQuB44PSIWlpO/p5c2SdIsmc5loFcC\n/wa8OCLGIuLCSbpfB+wCRoG/BN4CkJmPAB8Abi+395c2SdIsmfIcQGaeN8X24ZblBC46QL9NwKYO\nxydJOkj8JLAkVcoAkKRKGQCSVKk5922g/ea3i0qqlTMASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCS\nVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlpgyAiNgUEQ9H\nxPaWtt+LiPsi4u6I+GxELGjZ9t6IGI2IL0fEa1vaV5a20YhY2/+nIknqxHRmAJ8EVk5o2wL8cGa+\nHPgK8F6AiDgBOBd4abnPxyJiXkTMA/4MOAM4ATiv9JUkzZIpAyAzvwA8MqHthszcV1ZvBZaU5VXA\nVZn57cy8HxgFVpTbaGbuyszvAFeVvpKkWdKP/xP4l4BPleXFNIGw31hpA9gzof3kPjz2wPH/GJY0\nV/R0Ejgi3gfsA67Y39SmW07S3q7mmojYGhFbx8fHexmeJGkSXQdARKwGXg/8QmbufzMfA5a2dFsC\n7J2k/Skyc0NmjmTmyNDQULfDkyRNoasAiIiVwMXAGzPz8ZZNm4FzI+LQiDgeWA78O3A7sDwijo+I\nQ2hOFG/ubeiSpF5MeQ4gIq4ETgUWRcQYsI7mqp9DgS0RAXBrZv5qZt4bEVcDX6I5NHRRZj5R6rwV\nuB6YB2zKzHsPwvORJE3TlAGQmee1ad44Sf8PAh9s034dcF1Ho5MkHTT9uApIB4lXFEk6mPwqCEmq\nlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEr5\nbaAV8dtFJbVyBiBJlXIGoK45o5DmNmcAklQpZwAaGNOZUUx3NuHsRJqaMwBJqtSUARARmyLi4YjY\n3tJ2RERsiYid5efC0h4R8dGIGI2IuyPipJb7rC79d0bE6oPzdCRJ0zWdQ0CfBP4UuLylbS1wY2au\nj4i1Zf1i4AxgebmdDFwKnBwRRwDrgBEggTsiYnNmPtqvJyIdTB5S0tPRlAGQmV+IiOEJzauAU8vy\nZcAtNAGwCrg8MxO4NSIWRMSxpe+WzHwEICK2ACuBK3t+BtIc1M/zHVK3uj0JfHRmPgiQmQ9GxFGl\nfTGwp6XfWGk7ULukHvV7duJspx79vgoo2rTlJO1PLRCxBlgDsGzZsv6NTNKsMFAGV7cB8FBEHFv2\n/o8FHi7tY8DSln5LgL2l/dQJ7be0K5yZG4ANACMjI21DQlKdBn22M9fCrtvLQDcD+6/kWQ1c29J+\nfrka6BTgsXKo6Hrg9IhYWK4YOr20SZJmyZQzgIi4kmbvfVFEjNFczbMeuDoiLgR2A+eU7tcBrwNG\ngceBCwAy85GI+ABwe+n3/v0nhCVJTzUTs4npXAV03gE2ndambwIXHaDOJmBTR6OTJB00fhJYkipl\nAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaA\nJFXKAJCkShkAklQpA0CSKmUASFKlDABJqlRPARAR74yIeyNie0RcGRE/GBHHR8RtEbEzIj4VEYeU\nvoeW9dGyfbgfT0CS1J2uAyAiFgNvB0Yy84eBecC5wIeBSzJzOfAocGG5y4XAo5n5QuCS0k+SNEt6\nPQQ0H3hmRMwHDgMeBH4auKZsvww4qyyvKuuU7adFRPT4+JKkLnUdAJn5VeD3gd00b/yPAXcA38jM\nfaXbGLC4LC8G9pT77iv9j+z28SVJvenlENBCmr3644HnAc8CzmjTNfffZZJtrXXXRMTWiNg6Pj7e\n7fAkSVPo5RDQq4H7M3M8M78LfAb4cWBBOSQEsATYW5bHgKUAZfvhwCMTi2bmhswcycyRoaGhHoYn\nSZpMLwGwGzglIg4rx/JPA74E3AycXfqsBq4ty5vLOmX7TZn5lBmAJGlm9HIO4Daak7nbgHtKrQ3A\nxcC7ImKU5hj/xnKXjcCRpf1dwNoexi1J6tH8qbscWGauA9ZNaN4FrGjT91vAOb08niSpf/wksCRV\nygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUM\nAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKtVTAETEgoi4JiLui4gdEfFjEXFERGyJiJ3l\n58LSNyLioxExGhF3R8RJ/XkKkqRu9DoD+GPgc5n5EuBEYAewFrgxM5cDN5Z1gDOA5eW2Bri0x8eW\nJPWg6wCIiOcCrwI2AmTmdzLzG8Aq4LLS7TLgrLK8Crg8G7cCCyLi2K5HLknqSS8zgOcD48AnIuKL\nEfHxiHgWcHRmPghQfh5V+i8G9rTcf6y0SZJmQS8BMB84Cbg0M18BfJPvH+5pJ9q05VM6RayJiK0R\nsXV8fLyH4UmSJtNLAIwBY5l5W1m/hiYQHtp/aKf8fLil/9KW+y8B9k4smpkbMnMkM0eGhoZ6GJ4k\naTJdB0Bmfg3YExEvLk2nAV8CNgOrS9tq4NqyvBk4v1wNdArw2P5DRZKkmTe/x/u/DbgiIg4BdgEX\n0ITK1RFxIbAbOKf0vQ54HTAKPF76SpJmSU8BkJl3AiNtNp3Wpm8CF/XyeJKk/vGTwJJUKQNAkipl\nAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaA\nJFXKAJCkShkAklQpA0CSKmUASFKlDABJqlTPARAR8yLiixHx92X9+Ii4LSJ2RsSnIuKQ0n5oWR8t\n24d7fWxJUvf6MQN4B7CjZf3DwCWZuRx4FLiwtF8IPJqZLwQuKf0kSbOkpwCIiCXAmcDHy3oAPw1c\nU7pcBpxVlleVdcr200p/SdIs6HUG8EfAe4DvlfUjgW9k5r6yPgYsLsuLgT0AZftjpb8kaRZ0HQAR\n8Xrg4cy8o7W5TdecxrbWumsiYmtEbB0fH+92eJKkKfQyA3gl8MaIeAC4iubQzx8BCyJifumzBNhb\nlseApQBl++HAIxOLZuaGzBzJzJGhoaEehidJmkzXAZCZ783MJZk5DJwL3JSZvwDcDJxduq0Gri3L\nm8s6ZftNmfmUGYAkaWYcjM8BXAy8KyJGaY7xbyztG4EjS/u7gLUH4bElSdM0f+ouU8vMW4BbyvIu\nYEWbPt8CzunH40mSeucngSWpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIG\ngCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKdR0AEbE0Im6OiB0R\ncW9EvKO0HxERWyJiZ/m5sLRHRHw0IkYj4u6IOKlfT0KS1LleZgD7gN/IzB8CTgEuiogTgLXAjZm5\nHLixrAOcASwvtzXApT08tiSpR10HQGY+mJnbyvL/ADuAxcAq4LLS7TLgrLK8Crg8G7cCCyLi2K5H\nLknqSV/OAUTEMPAK4Dbg6Mx8EJqQAI4q3RYDe1ruNlbaJEmzoOcAiIhnA38D/Hpm/vdkXdu0ZZt6\nayJia0RsHR8f73V4kqQD6CkAIuIHaN78r8jMz5Tmh/Yf2ik/Hy7tY8DSlrsvAfZOrJmZGzJzJDNH\nhoaGehmeJGkSvVwFFMBGYEdm/mHLps3A6rK8Gri2pf38cjXQKcBj+w8VSZJm3vwe7vtK4M3APRFx\nZ2n7TWA9cHVEXAjsBs4p264DXgeMAo8DF/Tw2JKkHnUdAJn5z7Q/rg9wWpv+CVzU7eNJkvrLTwJL\nUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRV\nygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSMx4AEbEyIr4cEaMRsXamH1+S1JjRAIiI\necCfAWcAJwDnRcQJMzkGSVJjpmcAK4DRzNyVmd8BrgJWzfAYJEnMfAAsBva0rI+VNknSDIvMnLkH\nizgHeG1m/nJZfzOwIjPf1tJnDbCmrL4Y+PI0Si8Cvt7HoQ5yvUEeW7/rDfLYBr3eII+t3/UGeWz9\nrjfdWsdl5tBUneb3Pp6OjAFLW9aXAHtbO2TmBmBDJ0UjYmtmjvQ+vMGvN8hj63e9QR7boNcb5LH1\nu94gj63f9fo9tpk+BHQ7sDwijo+IQ4Bzgc0zPAZJEjM8A8jMfRHxVuB6YB6wKTPvnckxSJIaM30I\niMy8Driuz2U7OmQ0x+sN8tj6XW+Qxzbo9QZ5bP2uN8hj63e9vo5tRk8CS5IGh18FIUmVMgAkqVIz\nfg5Amq6IuDwzz5/tcQBExEtoPrW+GEiay5c3Z+aOWR3YQdByhd7ezPx8RLwJ+HFgB7AhM787y+Nb\nAWRm3l6+SmYlcF85v6gOzPlzABHxEzRfMbE9M2/ossZLaP6wb8vM/21pX5mZn+ugztuBz2bmnik7\nPw1ExAuAn6H5bMc+YCdwZWY+1kWtiZcDB/BTwE0AmfnG3kbbvYi4GDiP5qtLxkrzEpo3yasyc32H\n9U4GdmTmf0fEM4G1wEnAl4APdfP69VNEXEGzc3gY8A3g2cBngNNo3jNWz+LY1tF8l9h8YAtwMnAL\n8Grg+sz84GyNbS6acwEQEf+emSvK8q8AFwGfBU4H/q6LP8a3lxo7gB8B3pGZ15Zt2zLzpA5qPQZ8\nE/gP4Erg05k53sl45oryur0B+CfgdcCdwKM0gfCWzLylw3rbaN4AP06zhx00r+G5AJn5T30c+wWZ\n+YkO+n8FeOnEPd+yp3xvZi7v8PHvBU4sl0VvAB4HrqF5gz0xM3+2k3oHeIwjM/O/urzv3Zn58oiY\nD3wVeF5mPhERAdyVmS/vdXzdioh7aP5ODwW+BixpCdLbZnNsMyUijsrMh/tSLDPn1A34Ysvy7cBQ\nWX4WcE8X9e4Bnl2Wh4GtNCHwpMea7thozqucDmwExoHPAauB5xyE1+IfO+z/XOB3gb8C3jRh28e6\neN3mleXDgFvK8rJOX7dyv2cA76TZq/uR0rbrIP0b2t1h//toPlo/sf044MtdPP6OluVtE7bd2UW9\n9cCisjwC7AJGgf8EfrKLetuBQ4CFwP8AR5T2H2wdewf1jgEupfkm4COB3y7/fq4Gju2w1hfbLffw\n2h1eXr/7gP8qtx2lbUEX9VZOqL0RuBv4a+DoLuodMeF2JPBA+d0c0Wm9ibe5eA7gGRGxkOYNI7Ls\nYWfmNyNiXxf15mU57JOZD0TEqcA1EXEczV5oJzIzvwfcANwQET9AM109D/h9YMrv5pgoIg40Awma\nPaFOfILmMM3fAL8UET9HEwTfBk7pdGw00/AnaPbGngOQmbvL8+5Ied0uiYhPl58P0cM5qoi4+0Cb\ngKM7LPfrwI0RsZPvf5nhMuCFwFu7GN72llnIXRExkplbI+JFQDfH18/MzP3/t8bvAT+fzfHxF9G8\n8XT61QEbad4Q5wHvAz4dEbto/o1c1cX4Pgn8A81O2s3AFcCZNOdU/pzOvhH4OxFxWGY+Dvzo/saI\nOBz4Xhdju5rmMOOpmfm1UusYmp22TwOv6bDeh2h2+gD+AHiQZqb8s8BfAGd1WO/rNEHeajGwjWam\n/PwO6z1Zrwky0zea9NsF3F9+HlPan013ewA3UfY4W9rmA5cDT3RY64B7vsAzu3y+T5Qx3tzm9n8d\n1rpzwvr7gH+h2avY1mGtd9Ds2WygebO4oLQPAV/ow+/5TJrj4d3e/yGagDxuwm2Y5uRmp/WeQfMG\n+HPA2WV5XpdjO5zmTfE/gNto3vR30RxOO7GLevcB88vyrRO2dTwrLvd7Hs2hH4AF5Tmv6LJW6177\n7gnbOvqbBQ49QPsi4GVdjO2AM7jJtk1yn20tyxP/3rp5f3o3TaC8rKXt/m5+D+1uc+4cwIFExGE0\nU6z7O7zfEmBflvSfsO2VmfkvHdR6UWZ+pZPHn0bN7cDPZObONtv2ZObSNnc7UK0dNMeyv9fSthp4\nD81hsOM6HNtLgR+iOQF/Xyf3PdgiYiPwicz85zbb/joz3zQLw5o4jufQ7MHNB8Yy86Eu67yNZi9z\nPfAqmjfs/Sdtn5+Zb+7PiLsTEXdl5oll+Xcy87datt2TmS+bxbHdAHweuGz/6x8RRwO/CLwmM1/d\nYb0x4A9pZpoXAS/I8ia7/9xKF2NcAlxCM/tcR3Meprc9//21ny4B8HQVEWfT7MU95WuxI+KszPzb\nDmp9BLghMz8/oX0l8CfZ4clMDY5y6PLXgBfRBMoe4G9pvm+rm0OjfRMR7wc+ki1X2JX2FwLrM/Ps\n2RkZlMPJa2kOQx1Vmh+i+ZLK9Zn5aIf11k1o+lhmjpfDSh/JHi5rjog30MzahzPzmG7rPKmmATB3\ndXo1y0zV0uAY9N/rII+v32PrR71ytdMLMnN7X+oZAHNXROzOzGWDVkuDY9B/r4M8vn6PbRDrzcWr\ngKrSz6tZ+nxljAbEoP9eB3l8/R7boNebyAAYfEcDr6X5kFWrAP51FmtpcAz673WQx9fvsQ16vScx\nAAbf39NcoXPnxA0Rccss1tLgGPTf6yCPr99jG/R6T67hOQBJqpNfBy1JlTIAJKlSBoAkVcoAkKRK\nGQCSVKn/B6pjpZwMr4zeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3ff8e6748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augcount = pd.DataFrame(aug_label)\n",
    "augcount[0].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generating matrix of images using cv2\n",
    "\n",
    "We will use cv2's imread function to read in the images into matrices followed by a normalisation step of dividing each pixel value by 255. Since pixel values range between 0-255, the normalisation step serves to eliminate any scaling issues while building the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-8\n",
      "\n",
      "Loaded the images of dataset-4\n",
      "\n",
      "Loaded the images of dataset-11\n",
      "\n",
      "Loaded the images of dataset-6\n",
      "\n",
      "Loaded the images of dataset-12\n",
      "\n",
      "Loaded the images of dataset-15\n",
      "\n",
      "Loaded the images of dataset-0\n",
      "\n",
      "Loaded the images of dataset-10\n",
      "\n",
      "Loaded the images of dataset-5\n",
      "\n",
      "Loaded the images of dataset-3\n",
      "\n",
      "Loaded the images of dataset-14\n",
      "\n",
      "Loaded the images of dataset-7\n",
      "\n",
      "Loaded the images of dataset-13\n",
      "\n",
      "Loaded the images of dataset-16\n",
      "\n",
      "Loaded the images of dataset-9\n",
      "\n",
      "Loaded the images of dataset-1\n",
      "\n",
      "Loaded the images of dataset-2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "# Read image using cv2, then append matrix to list\n",
    "\n",
    "img_data_list=[]\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "        input_img_resize=cv2.resize(input_img,(128,128))\n",
    "        img_data_list.append(input_img_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18337, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Make matrix into arrays\n",
    "\n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Shuffling then train test split the dataset\n",
    "\n",
    "After reading in the images into matrices. The matrix is shuffled records-wise to reduce bias and split into training and test sets (70% training, 30% test split)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
